This is a soccer shooting practice game. Our team consists of one goalkeeper and two offensive players (i.e., three reinforcement learning agents), while the opposing team has one goalkeeper and one defender.
The objective is to score goals by cooperating through actions such as passing and positioning to bring the ball as close as possible to the opponent's goal, located at coordinates (1.0, 0.0), and eventually score.

The agents receive rewards consisting of two parts: progress reward and goal reward.
The progress reward provides a team reward when one of our agents carries the ball closer to the opponent’s goal. This reward is only granted if a player on our team is in possession of the ball and the distance between the ball and the opponent's goal is less than 0.99 and greater than 0.19. The player in possession is typically the one closest to the ball.
The goal reward is a team reward given when a goal is successfully scored. An additional bonus is awarded based on the distance from which the shot was taken—the farther the distance, the higher the bonus.

Each reinforcement learning agent receives a local observation of 115 dimensions:
0–5: Positions of our players: 3 players, each with (x, y)
6–11: Directions of our players: 3 players, each with (x, y)
12–15: Positions of opponent players: 2 players, each with (x, y)
16–19: Directions of opponent players: 2 players, each with (x, y)
20–87: Invalid information
88–90: Ball position: (x, y, z)
91–93: Ball direction: (x, y, z)
94–96: Ball possession: our team: [0, 1, 0], opponent team: [0, 0, 1], no one in control: [1, 0, 0]
97–114: Invalid information

The global state vector is constructed by concatenating the local observations of the three controlled agents, resulting in a total of 345 dimensions.